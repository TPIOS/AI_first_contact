#### 问题描述：

给予一个train训练集，带有每个单词和其所对应的词性，对于一个新的test，判断该词的词性

#### 目前研究成果：

 https://aclweb.org/aclwiki/POS_Tagging_(State_of_the_art) 

POS Tagging Accuracies • Rough accuracies: 

• Baseline: most freq tag: ~90% 

• Trigram HMM: ~95% 

• Maxent P(t|w): 93.7% 

• MEMM tagger: 96.9% 

• Bidirectional MEMM: 97.2% 

• Upper bound: ~98% (human agreement) 

#### 个人理解：

简单来讲就是$P(Tag|Word)$，也就是在给予该word的情况下，它所有可能的tag的概率是多少，然后从中选取最高的tag

$argmax_{Tag}(P(Tag|Word))$

单纯计算的话，$P(Tag|Word)=\frac{P(Tag, Word)}{P(Word)}$，其实也就是在问$P(Tag,Word)$。可以有进一步的转化$P(Tag,Word)=P(Word|Tag)*P(Tag)$

这样转换是为了HMM做准备，因为直接只通过一个单词去判断其词性，显然会丢失很多句子层面上的信息，这个单词或者这个单词词性的前面是谁，后面是谁，前面的前面是谁，后面的后面是谁，等等，都有可能跟这个单词的词性有密切联系，那么显然我们需要通过训练来得到有关单词词性前后的信息。

HMM模型告诉我们，假如说对于一个词，已知这个词前一个词的词性，那么计算这个词的词性，可以是$P(这个词词性|上一个词词性)*P(这个单词|这个词词性)$

#### 最大困难：

通过训练集得出对于训练集的$P(这个词词性|上一个词词性)*P(这个单词|这个词词性)$，$P(这个词词性|下一个词词性)*P(这个单词|这个词词性)$ 这些都不难统计和计算，但是对于未知的test，最难的问题就是当$C(这个词词性，下一个词词性)$或者$C(这个词词性，上一个词词性)$这样的组合从来没有出现，以及单词本身就不在train的词库里，自然$C(单词，词性)$这个组合也就出现不了，如果不做任何Smoothing处理，那么会导致对于一个生单词来讲，它所有的概率都将是0，这显然会导致一个句子bug的出现，所以smoothing处理是必然的。

对于词性来讲，通常的smoothing方式足以应对，但是对于未收录词语UNK，事情就变得稍微复杂了许多。

#### 个人解法：

##### 1. 最简单的add-one smoothing:

其实简单模仿一下HMM正向，就可以做一个双向的，比如说$P(这个词词性|下一个词词性)*P(这个单词|这个词词性)$

这样就可以用到前后两个方向的单词词性，作为判断一个单词的标准

然后跑两遍Viterbi算法，计算出对应一个句子它最佳的词性序列，一个正向，一个负向，对比，若两个序列对一个词标明的为同一词性则该词词性确定，若两个序列对一个词的标明有区别，取最大值，这样会导致更靠近句首的词更依赖前面，靠近句末的词更依赖后面。

一种效率非常低的方式首先想到，遍历test，提取其中所有的UNK，然后假设该UNK对于任意词性都出现过1次，这样简单的add-one，但是这样会导致比如说单词`Hahn`是NNP是8/93470，而对于FW出现次数仅有0/238，但是由于我分别多加了一次，就变成了9/93470对1/238，变成了按照概率显然后者要比前者出现的可能要大，这是不允许的，所以对于word使用**add-one smoothing**显然不好，于是放弃smoothing，改为直接预测，对于UNK，则使用看前面的单词词性直接计算最优后面接着的词性

还需要考虑log prob的问题

可以对单词的复数或者单数进行处理吗？

这个单纯的方法能得到：

0.9419520080616379

存在一些answer有错误的情况？我将我错误的答案进行了输出，发现了一些：

1. `House-Senate`在answer中只有NNP的形式，但是在train中有JJ的形式，然而理论上在test这个样例中，house-senate确实应该位JJ，所以这样的错误肯定不可避免。
2. `the/DT House/NNP Appropriations/NNP Committee/NNP`这个其中的Appropriations在train中就是NNP，但是在answer中却成了NNPS，说明answer也确有问题
3. 错的比较多的都是`JJ, VBG`, `VBZ, NNS`, `NN VBG`，`JJ, VBN`
4. 数字CD
5. 开头大写的字母且不在词库里的，尾巴又没有s的，非常有可能是NNP
6. insert竟然没有动词形式
7. 中间大写字母果然需要单独考虑

暂且把目前的94.19%正确率叫做version0.5，接下来开始陆续更新

---

0.9436105221170197

主要做了对数字的处理，以及对`-`的处理，还有句首单词大小写的处理，还有单复数，现在准确度是94.36%

发现问题：

1. 单复数处理不恰当，像是railcar这个词，在train中有railcars是NNS，但还有Railcar是NNP，所以对于railcar来说他应该既有Railcar的NNP形式，也应该有railcars去掉复数的单数形式
2. 过去时的生单词

---

0.9445552453131233

对专有名词处理了一下，过去时和过去分词增加了一些

发现问题：

1. 名词形容词化很多，过去时形容词化很多，动名词形容词化也很多，JJ很容易跟其他混淆
2. 这次凡是executive我都错了

这些都是暂时性处理单词，并不是从模型上加以改进，我觉得对于Bigram来讲，94.45%应该已经不错，所以来想想Tri-gram的HMM模型吧

算法时间复杂度来讲，bigram的算法耗时为$O(L*T^2)$，那么trigram算法耗时为$O(L*T^3)$

同理的是，更多条记录会重新开始，为了不破坏已经搭建好的bigram模型，现另开一个trigram模型

只不过trigram的backpoint有一点让我很费解，backpoint应该是只能记录一个前置位置，不可能直接记录两个前置位置，因为如果记录前两个最优位置，那么往前的跳跃会过快，那么只能记录一个

---

教授说先让我把bi-gram模型争取弄到96以上，再考虑tri-gram，所以我们还是优化bi-gram的后期处理，因为准确率已经逼近95%，那么viterbi算法应该是没有出错，现在影响准确率的就是单词的ambiguity了

94.26%，增加了NNPS之后，准确率下降了

89.31%，失败的尝试？

把很多东西都删掉了，感觉最重要的就是buildTragger.py里面if i == 0: word = word.lower()这句话，如果

改成这样的，if tag != "NNP" or tag != "NNPS": word = word.lower()，效果立马下降

Accuracy= 0.9452060546259946

---

误操作53.49%可还行

Accuracy= 0.9480402242143052

多加了ing和ment还有ness的判定，将这些判定移动到判断单词是否已收录之前。因为这种特殊suffix结尾的几乎已经确定是某种词性的单词可以提前判断，就像数字一样

---

Accuracy= 0.9483341380975374

判断顺序非常影响

哪种顺序应该更早分类，哪种顺序应该稍后分类

---

Accuracy= 0.9486280519807696

增加了对有可能是正常单词但是大写了的可能性

---

去掉了+"s"的判断

Accuracy竟然直接达到了0.9541074465181701

---

突然想到的是，假如说这个单词所有词性都不适合，也许这个单词有隐含词性，那么我们直接当这个单词不认识走

把全是0的都去掉了，现在准确率是：0.9566686960720509，

强制性数字：0.95702559150169

Accuracy= 0.9570885730480969

---

Accuracy= 0.9564377637352256

Accuracy= 0.9567736653160623

Accuracy= 0.9574874561753406

扩充了动词词库

---

修正了动词词库的扩展；修改了一处没有return x的bug

Accuracy= 0.9580752839418051

增加了s结尾是VBZ的情况：Accuracy= 0.9582432347322235

---

放弃NNPS的判定（把es结尾的规定为NNPS），全部都赋值成NNP，Accuracy= 0.9591459702307223

hard-code了Sept.30，对-year-old结尾进行了特殊处理，对dash进行了维护，

Accuracy= 0.9592509394747339

对upper增加了额外的处理：Accuracy= 0.9592929271723385

---



